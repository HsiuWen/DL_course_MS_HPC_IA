{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6084c39e-0521-4d56-8b68-3b42d0b69d8d",
   "metadata": {},
   "source": [
    "# Cycle-GAN for Horse-to-Cat Style Transfer\n",
    "\n",
    "In this exercise you will try to make horses look like cats. Because this is one of the more difficult exercises a maximum amount of code is already given. However, you still need to add some code and do training and evalluation.\n",
    "\n",
    "## Tasks\n",
    "1. Add all necessary code to train a Cycle GAN. This includes especially data loaders.\n",
    "2. Train and optimize the Cycle GAN. This step is not trivial and you should start with very small image resolutions to get a feeling for hyperparameters\n",
    "3. Visualize generated images. For this task no quantifyable measure apart from the loss can be given. Qualitative examples of horse images which have been turned to cats should be shown.\n",
    "\n",
    "**Important**: At the end you should write a report of adequate size, which will probably mean at least half a page. In the report you should describe how you approached the task. You should describe:\n",
    "- Encountered difficulties (due to the method, e.g. \"not enough training samples to converge\", not technical like \"I could not install a package over pip\")\n",
    "- Steps taken to alleviate difficulties\n",
    "- General description of what you did, explain how you understood the task and what you did to solve it in general language, no code.\n",
    "- Potential limitations of your approach, what could be issues, how could this be hard on different data or with slightly different conditions\n",
    "- If you have an idea how this could be extended in an interesting way, describe it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744323ec-6016-4ddd-84e1-cbb86c235a15",
   "metadata": {},
   "source": [
    "### Some Information \n",
    "### GAN (Generative Adversarial Network)\n",
    "In a generative adversarial network (GAN) two neural networks contest with each other to generate data (Generator) and to guess if it's generated or true data (Discriminator. One networks's gain is another agent's loss.\n",
    "\n",
    "Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proved useful for semi-supervised learning, fully supervised learning and reinforcement learning.\n",
    "\n",
    "The core idea of a GAN is based on the \"indirect\" training through the discriminator which is a neural network that can tell how \"realistic\" the input seems, which itself is also being updated dynamically. This means that the generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator. This enables the model to learn in an unsupervised manner.\n",
    "\n",
    "#### Cycle GAN\n",
    "A CycleGAN is an architecture for performing translations between two domains, such as between photos of horses and photos of zebras, or photos of night cities and photos of day cities. Unlike previous work like pix2pix, which requires paired training data, cycleGAN requires no paired data. For example, to train a pix2pix model to turn a summer scenery photo to winter scenery photo and back, the dataset must contain pairs of the same place in summer and winter, shot at the same angle; cycleGAN would only need a set of summer scenery photos, and an unrelated set of winter scenery photos.\n",
    "                                                                                                                                            \n",
    "Modified after https://en.wikipedia.org/wiki/Generative_adversarial_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb2fd77-f61e-467f-9514-816e082c98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dataset Loading and Preprocessing\n",
    "# -------------------\n",
    "# Instructions:\n",
    "# - Download and prepare a dataset that contains images of horses and cats. You can choose datasets like MS COCO, Pascal VOC, or Tiny Imagenet. \n",
    "# - Use the pycocotools library for loading the MS COCO dataset, or implement a custom data loader if you use another dataset.\n",
    "# - Preprocess the images: resize them to a smaller size (e.g., 32x32) for faster training and normalize the pixel values.\n",
    "# - Create a custom Dataset class for loading horse and cat images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc9232-c18f-4e6b-808b-917809db9d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda06d3-dfd4-4e35-a379-1cb71e23a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your dataset (adjust for your chosen dataset)\n",
    "dataDir = './datasets/coco/'\n",
    "dataType = 'train2017'\n",
    "coco = COCO(os.path.join(dataDir, 'annotations', 'instances_' + dataType + '.json'))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# Here is a custom dataset class that resizes images to 32x32. You will need this dimension to run fast enough\n",
    "# 1. Dataset Loading and Preprocessing\n",
    "# -------------------\n",
    "# Download and prepare the MS COCO dataset (or use a similar dataset) containing horse and cat images\n",
    "# We'll use pycocotools for MS COCO dataset handling. Images will be resized to 32x32 for faster training.\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, coco, category_id, transform=None):\n",
    "        self.coco = coco\n",
    "        self.category_id = category_id\n",
    "        self.transform = transform\n",
    "        self.img_ids = list(coco.getImgIds(catIds=[category_id]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs([img_id])[0]\n",
    "        img_path = os.path.join(dataDir, dataType, img_info['file_name'])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Resize the image to 32x32\n",
    "        img = img.resize((32, 32)) \n",
    "        \n",
    "        \n",
    "        if self.transform:            \n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img\n",
    "\n",
    "# Define the transformation pipeline for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Create datasets for horses and cats (category IDs may vary)\n",
    "horse_category_id = 24  # Example category for 'horse'\n",
    "cat_category_id = 17    # Example category for 'cat'\n",
    "\n",
    "horse_dataset = ImageDataset(coco, horse_category_id, transform=transform)\n",
    "cat_dataset = ImageDataset(coco, cat_category_id, transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "horse_loader = DataLoader(horse_dataset, batch_size=32, shuffle=True)\n",
    "cat_loader = DataLoader(cat_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2e160-b8ae-4975-94cf-66d107755db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Cycle-GAN Model Architecture\n",
    "# -------------------\n",
    "# Instructions:\n",
    "# - Implement the Generator and Discriminator models for Cycle-GAN using PyTorch.\n",
    "# - The Generator should use a U-Net like architecture for image-to-image translation.\n",
    "# - The Discriminator should classify whether an image is real or fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac2838d-8549-43cd-a6cc-5d42ea0272fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Model\n",
    "# Define the architecture for the Generator. This should include downsampling and upsampling blocks to transform images from one domain (horse) to another (cat).\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Build a simple U-Net like architecture with convolutional layers\n",
    "        pass  # Replace this with your implementation\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass through the Generator network\n",
    "        pass  # Replace this with your implementation\n",
    "\n",
    "# Discriminator Model\n",
    "# Define the architecture for the Discriminator. This model will classify images as real or fake.\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Build a series of convolutional layers for the Discriminator\n",
    "        pass  # Replace this with your implementation\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass through the Discriminator network\n",
    "        pass  # Replace this with your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa43f5-5167-44e1-bc25-55eaa24a4010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Loss Functions\n",
    "# -------------------\n",
    "# Instructions:\n",
    "# - Implement the adversarial loss and cycle consistency loss.\n",
    "# - The adversarial loss (for the generator) should be based on Binary Cross-Entropy (BCE).\n",
    "# - The cycle consistency loss should be L1 loss, ensuring that after translating from one domain to the other and back again, the image is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899c3e8-9f3e-4c1e-af87-a005b781eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_loss(real, fake):\n",
    "    # Implement the adversarial loss function\n",
    "    pass  # Replace this with your implementation\n",
    "\n",
    "def cycle_loss(real, reconstructed):\n",
    "    # Implement the cycle consistency loss function\n",
    "    pass  # Replace this with your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032b069e-b141-414b-8fea-9fbfc4ff9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Training Loop\n",
    "# -------------------\n",
    "# Instructions:\n",
    "# - Initialize the Generator and Discriminator models.\n",
    "# - Define optimizers (Adam optimizer with a learning rate of 0.0002 and betas (0.5, 0.999)).\n",
    "# - Implement the training loop for both Generators and Discriminators.\n",
    "# - Train the Generators using adversarial loss and cycle loss.\n",
    "# - Train the Discriminators using real and fake images.\n",
    "# - Print the loss values during the training for monitoring the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7157a8c-274a-4cee-b4b4-1c6720dcc123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "G_A_to_B = Generator().to(device)\n",
    "G_B_to_A = Generator().to(device)\n",
    "D_A = Discriminator().to(device)\n",
    "D_B = Discriminator().to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(list(G_A_to_B.parameters()) + list(G_B_to_A.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_A = optim.Adam(D_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_B = optim.Adam(D_B.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd40a6-7822-4825-8246-3c697989e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# Instructions: Implement the loop where the model trains for a number of epochs.\n",
    "# - In each epoch, generate fake images using the Generator models.\n",
    "# - Calculate adversarial and cycle losses.\n",
    "# - Update the weights of the Generators and Discriminators using backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe83d18-c89a-40b0-bfdd-561159481ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (horse_real, cat_real) in enumerate(zip(horse_loader, cat_loader)):\n",
    "        # Move data to device\n",
    "        horse_real = horse_real.to(device)\n",
    "        cat_real = cat_real.to(device)\n",
    "\n",
    "        # Train Generators\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate fake images\n",
    "        horse_fake = G_A_to_B(horse_real)\n",
    "        cat_fake = G_B_to_A(cat_real)\n",
    "\n",
    "        # Adversarial loss\n",
    "        loss_G_A_to_B = adversarial_loss(D_B(horse_fake), torch.ones_like(D_B(horse_fake)))\n",
    "        loss_G_B_to_A = adversarial_loss(D_A(cat_fake), torch.ones_like(D_A(cat_fake)))\n",
    "\n",
    "        # Cycle loss\n",
    "        horse_reconstructed = G_B_to_A(horse_fake)\n",
    "        cat_reconstructed = G_A_to_B(cat_fake)\n",
    "        loss_cycle_A = cycle_loss(horse_real, horse_reconstructed)\n",
    "        loss_cycle_B = cycle_loss(cat_real, cat_reconstructed)\n",
    "\n",
    "        # Total generator loss\n",
    "        loss_G = loss_G_A_to_B + loss_G_B_to_A + loss_cycle_A + loss_cycle_B\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Train Discriminators\n",
    "        optimizer_D_A.zero_grad()\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # Real and fake discriminators\n",
    "        loss_D_A_real = adversarial_loss(D_A(cat_real), torch.ones_like(D_A(cat_real)))\n",
    "        loss_D_A_fake = adversarial_loss(D_A(cat_fake.detach()), torch.zeros_like(D_A(cat_fake)))\n",
    "        loss_D_A = (loss_D_A_real + loss_D_A_fake) / 2\n",
    "\n",
    "        loss_D_B_real = adversarial_loss(D_B(horse_real), torch.ones_like(D_B(horse_real)))\n",
    "        loss_D_B_fake = adversarial_loss(D_B(horse_fake.detach()), torch.zeros_like(D_B(horse_fake)))\n",
    "        loss_D_B = (loss_D_B_real + loss_D_B_fake) / 2\n",
    "\n",
    "        loss_D_A.backward()\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_A.step()\n",
    "        optimizer_D_B.step()\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] | Loss_G: {loss_G.item()} | Loss_D_A: {loss_D_A.item()} | Loss_D_B: {loss_D_B.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e1e59-ace4-4552-b05c-e155a4552c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualization\n",
    "# -------------------\n",
    "# Instructions:\n",
    "# - After training, visualize some generated images.\n",
    "# - Display the real and generated images for both the horse-to-cat and cat-to-horse transformations.\n",
    "# - You should visualize at least 15-20 examples to evaluate the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0e9e8e-06aa-43b8-bc75-16a70827951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_real_and_fake_examples(horse_loader, cat_loader, generator_horse_to_cat, generator_cat_to_horse, n_examples=5, device='cuda'):\n",
    "    \"\"\"\n",
    "    Visualizes n_examples of real and fake animals using the provided model.\n",
    "    \n",
    "    Args:\n",
    "    - horse_loader (DataLoader): Dataloader for horse images.\n",
    "    - cat_loader (DataLoader): Dataloader for cat images.\n",
    "    - generator_horse_to_cat (nn.Module): Generator model to convert horses to cats.\n",
    "    - generator_cat_to_horse (nn.Module): Generator model to convert cats to horses.\n",
    "    - n_examples (int): Number of examples to visualize.\n",
    "    - device (str): Device to run the models on ('cuda' or 'cpu').\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    generator_horse_to_cat.eval()\n",
    "    generator_cat_to_horse.eval()\n",
    "    \n",
    "    # Create the figure for visualization\n",
    "    fig, axs = plt.subplots(n_examples, 4, figsize=(12, 3 * n_examples))\n",
    "    \n",
    "    # Loop through the number of examples\n",
    "    for i in range(n_examples):\n",
    "        # Get a batch of horse and cat images\n",
    "        horse_real = next(iter(horse_loader)).to(device)\n",
    "        cat_real = next(iter(cat_loader)).to(device)\n",
    "        \n",
    "        # Generate fake images\n",
    "        horse_fake = generator_horse_to_cat(horse_real)\n",
    "        cat_fake = generator_cat_to_horse(cat_real)\n",
    "        \n",
    "        # Plot real images (Horse -> Cat)\n",
    "        axs[i, 0].imshow(horse_real[0].permute(1, 2, 0).cpu().detach().numpy())\n",
    "        axs[i, 0].set_title(\"Real Horse\")\n",
    "        axs[i, 0].axis('off')\n",
    "        \n",
    "        axs[i, 1].imshow(cat_fake[0].permute(1, 2, 0).cpu().detach().numpy())\n",
    "        axs[i, 1].set_title(\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
