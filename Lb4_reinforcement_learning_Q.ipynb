{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lb4_reinforcement_learning_Q.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TleCCL460AqV"
      },
      "source": [
        "# Lab4 Implementation of q-learning on simulated game\r\n",
        "\r\n",
        "This notebook has been prepared by Hsiu-Wen Chang from MINES ParisTech\r\n",
        "Shall you have any problem, send me [email](hsiu-wen.chang_joly@mines-paristech.fr)\r\n",
        "\r\n",
        "In this practical lesson, we are going to implement the reinforcement learning using Q-learning. We are going to train it on how to play a mountain car. The agent (a car) is started at the bottom of a valley. For any given state the agent may choose to accelerate to the left, right or cease any acceleration.\r\n",
        "        \r\n",
        "\r\n",
        "1. The observation space contains two values (car position and car velocity)\r\n",
        "2. The available actions are:\r\n",
        "    0: push left\r\n",
        "    1: nothing\r\n",
        "    2: push right"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7hCi7AVDo-2"
      },
      "source": [
        "!apt-get install x11-utils > /dev/null 2>&1\r\n",
        "!pip install pyglet==v1.3.2\r\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\r\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\r\n",
        "from IPython import display as ipythondisplay\r\n",
        "from pyvirtualdisplay import Display\r\n",
        "display = Display(visible=0, size=(400, 300))\r\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3TwgkwazweP"
      },
      "source": [
        "import gym\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "env = gym.make('MountainCar-v0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTbOEoEq4ro9"
      },
      "source": [
        "Now we need to define our search space for the q matrix. Here we use the range of observation_space and divided by resolution 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flxxoUZi2Xu2"
      },
      "source": [
        "DISCRETE_OS_SIZE = [20]*len(env.observation_space.high)\r\n",
        "discrete_os_win_size = (env.observation_space.high-env.observation_space.low)/DISCRETE_OS_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT0EdWaO52Li"
      },
      "source": [
        "Here are the parameters that control the way to learn. \r\n",
        "We apply the idea of epsilon decay that help use to decrease the chance to do random behavior for exploration. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r0I7VT_2P5d"
      },
      "source": [
        "# training parameter\r\n",
        "LEARNING_RATE = 0.1\r\n",
        "DISCOUNT = 0.95 # the weight of how to meausre the future reward vs current reward. [0.1]\r\n",
        "EPISODES = 4000 # training epochs\r\n",
        "SHOW_EVERY = 500 #show information every 500 loops\r\n",
        "\r\n",
        "epsilon = 0.5 #[0~1] higher value for higher random for exploration behavior\r\n",
        "START_EPSILON_DECAYING = 1\r\n",
        "END_EPSILON_DECAYING = EPISODES // 2\r\n",
        "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING-START_EPSILON_DECAYING)\r\n",
        "\r\n",
        "# Now we create a Q matrix with random values\r\n",
        "q_table = np.random.uniform(low=-2,high=0,size = (DISCRETE_OS_SIZE+[env.action_space.n]))\r\n",
        "print(q_table.shape)\r\n",
        "ep_rewards = []\r\n",
        "aggr_ep_rewards = {'ep':[],'avg':[],'min':[],'max':[]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVTHJFea5xvC"
      },
      "source": [
        "To simply the task, we discrete the state to match the q-matrix we have"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur5nYXYQ4_pV"
      },
      "source": [
        "def get_discrete_state(state):\r\n",
        "    discrete_state = (state-env.observation_space.low)/discrete_os_win_size\r\n",
        "    return tuple(discrete_state.astype(np.int))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvJcY2w-8u3J"
      },
      "source": [
        "# Create a folder to save the q table\r\n",
        "!mkdir qtables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIHweObp64X2"
      },
      "source": [
        "# Now we start to train the agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL8Zz3hR63XZ"
      },
      "source": [
        "for episode in range(EPISODES):\r\n",
        "    episode_reward = 0\r\n",
        "\r\n",
        "    # Don't do render in Colab\r\n",
        "    render = False\r\n",
        "\r\n",
        "    #if episode % SHOW_EVERY == 0 :\r\n",
        "    #    print(episode)\r\n",
        "    #    render = True\r\n",
        "    #else:\r\n",
        "    #    render = False\r\n",
        "\r\n",
        "    # Restart the game\r\n",
        "    discrete_state = get_discrete_state(env.reset())\r\n",
        "\r\n",
        "    done = False\r\n",
        "    while not done:\r\n",
        "        # Take the action randomly if the we pick up a random number bigger than epsilon, otherwise we use q table to see which action has the highest value\r\n",
        "        if np.random.random() > epsilon:\r\n",
        "            action = np.argmax(q_table[discrete_state])\r\n",
        "        else:\r\n",
        "            action = np.random.randint(0,env.action_space.n)\r\n",
        "\r\n",
        "        # Given this new action a, we act with the environment to get the new state,our reward and flag to tell us if the game finish\r\n",
        "        new_state, reward, done, _ = env.step(action)\r\n",
        "\r\n",
        "        # Accumuated reward\r\n",
        "        episode_reward += reward\r\n",
        "\r\n",
        "        new_discrete_state = get_discrete_state(new_state)\r\n",
        "\r\n",
        "        if render:\r\n",
        "            env.render()\r\n",
        "\r\n",
        "        if not done:\r\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\r\n",
        "            current_q = q_table[discrete_state + (action,)]\r\n",
        "            new_q = (1-LEARNING_RATE)*current_q+LEARNING_RATE*(reward+DISCOUNT*max_future_q)\r\n",
        "            q_table[discrete_state+(action,)] = new_q\r\n",
        "        elif new_state[0] >= env.goal_position:\r\n",
        "            print(f\"We made it on episode {episode}\")\r\n",
        "            q_table[discrete_state+(action,)]=0\r\n",
        "\r\n",
        "        discrete_state = new_discrete_state\r\n",
        "\r\n",
        "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\r\n",
        "        epsilon -= epsilon_decay_value\r\n",
        "\r\n",
        "    ep_rewards.append(episode_reward)\r\n",
        "\r\n",
        "    if not episode % SHOW_EVERY:\r\n",
        "        np.save(f'qtables/{episode}-qtable.npy',q_table)\r\n",
        "        average_reward = sum(ep_rewards[-SHOW_EVERY:])/len(ep_rewards[-SHOW_EVERY:])\r\n",
        "        aggr_ep_rewards['ep'].append(episode)\r\n",
        "        aggr_ep_rewards['avg'].append(average_reward)\r\n",
        "        aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\r\n",
        "        aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\r\n",
        "        \r\n",
        "        print(f\"Episode: {episode} average reward:{average_reward} min reward:{min(ep_rewards[-SHOW_EVERY:])} max reward:{max(ep_rewards[-SHOW_EVERY:])}\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uMG1ioY_HKp"
      },
      "source": [
        "We can see the history about the reward "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCToSS9X7C06"
      },
      "source": [
        "env.close()\r\n",
        "\r\n",
        "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['avg'],label='avg')\r\n",
        "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['min'],label='min')\r\n",
        "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['max'],label='max')\r\n",
        "plt.legend(loc=4)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vULE9_RAcAa"
      },
      "source": [
        "Task: Try to play with the parameter. For example\r\n",
        "\r\n",
        "1. epsilon like END_EPSILON_DECAYING and see if you can make the training better\r\n",
        "2. Resolution for the DISCRETE_OS_SIZE\r\n",
        "3. Increase the epochs "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJXxrkaQBg7N"
      },
      "source": [
        "# Visualization\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJDfbWvWIeix"
      },
      "source": [
        "If you run gym on Colab, you can't just use render function to see the game. Here is a tricky way to plot it in notebook.\r\n",
        "\r\n",
        "You can revise the code to see the behavior of your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecIlAPzSEu16"
      },
      "source": [
        "prev_screen = env.render(mode='rgb_array')\r\n",
        "plt.imshow(prev_screen)\r\n",
        "\r\n",
        "for i in range(50):\r\n",
        "  action = env.action_space.sample()\r\n",
        "  obs, reward, done, info = env.step(action)\r\n",
        "  screen = env.render(mode='rgb_array')\r\n",
        "  \r\n",
        "  plt.imshow(screen)\r\n",
        "  ipythondisplay.clear_output(wait=True)\r\n",
        "  ipythondisplay.display(plt.gcf())\r\n",
        "\r\n",
        "  if done:\r\n",
        "    break\r\n",
        "\r\n",
        "ipythondisplay.clear_output(wait=True)\r\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4tYdOZgIzFG"
      },
      "source": [
        "Here is code that shows us the Q table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nv7NT3a_cPX"
      },
      "source": [
        "from mpl_toolkits.mplot3d import axes3d\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from matplotlib import style\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "style.use('ggplot')\r\n",
        "\r\n",
        "def get_q_color(value, vals):\r\n",
        "    if value == max(vals):\r\n",
        "        return \"green\", 1.0\r\n",
        "    else:\r\n",
        "        return \"red\", 0.3\r\n",
        "\r\n",
        "\r\n",
        "fig = plt.figure(figsize=(12, 9))\r\n",
        "\r\n",
        "ax1 = fig.add_subplot(311)\r\n",
        "ax2 = fig.add_subplot(312)\r\n",
        "ax3 = fig.add_subplot(313)\r\n",
        "\r\n",
        "i = 3500\r\n",
        "q_table = np.load(f\"qtables/{i}-qtable.npy\")\r\n",
        "\r\n",
        "\r\n",
        "for x, x_vals in enumerate(q_table):\r\n",
        "    for y, y_vals in enumerate(x_vals):\r\n",
        "        ax1.scatter(x, y, c=get_q_color(y_vals[0], y_vals)[0], marker=\"o\", alpha=get_q_color(y_vals[0], y_vals)[1])\r\n",
        "        ax2.scatter(x, y, c=get_q_color(y_vals[1], y_vals)[0], marker=\"o\", alpha=get_q_color(y_vals[1], y_vals)[1])\r\n",
        "        ax3.scatter(x, y, c=get_q_color(y_vals[2], y_vals)[0], marker=\"o\", alpha=get_q_color(y_vals[2], y_vals)[1])\r\n",
        "\r\n",
        "        ax1.set_ylabel(\"Action 0\")\r\n",
        "        ax2.set_ylabel(\"Action 1\")\r\n",
        "        ax3.set_ylabel(\"Action 2\")\r\n",
        "\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n15R9SCaBnzA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}