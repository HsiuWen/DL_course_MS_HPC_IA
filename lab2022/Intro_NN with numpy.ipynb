{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Lab 2 Implementation of MLPs from scratch\n",
    "This notebook has been prepared by Hsiu-Wen Chang from MINES ParisTech Shall you have any problem, send me [email](hsiu-wen.chang_joly@mines-paristech.fr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understand and experiment MLP on a VERY simple classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "# Author: Pr Fabien MOUTARDE, Center for Robotics, MINES ParisTech, PSL Research University\n",
    "###########################################################################################\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Create artificial dataset (classification problem within 2 classes within R^2 input space)\n",
    "Xmoon, y_moon = make_moons(n_samples=900, noise=0.2, random_state=0)\n",
    "\n",
    "# Preprocess dataset, and split into training and test part\n",
    "Xmoon = StandardScaler().fit_transform(Xmoon)\n",
    "Xmoon_train, Xmoon_test, y_moon_train, y_moon_test = train_test_split(Xmoon, y_moon, test_size=0.7)\n",
    "\n",
    "# Encode class labels as binary vector (with exactly ONE bit set to 1, and all others to 0)\n",
    "Ymoon_train_OneHot = np.eye(2)[y_moon_train]\n",
    "Ymoon_test_OneHot = np.eye(2)[y_moon_test]\n",
    "\n",
    "# Print beginning of training dataset (for verification)\n",
    "print(\"Number of training examples = \", y_moon_train.size)\n",
    "print()\n",
    "print(\"  first \", round(y_moon_train.size/10), \"training examples\" )\n",
    "print(\"[  Input_features  ]     [Target_output]\")\n",
    "for i in range( int(round(y_moon_train.size/10) )):\n",
    "    print( Xmoon_train[i], Ymoon_train_OneHot[i])\n",
    "\n",
    "# Plot training+testing dataset\n",
    "################################\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "# Plot the training points...\n",
    "plt.scatter(Xmoon_train[:, 0], Xmoon_train[:, 1], c=y_moon_train, cmap=cm_bright)\n",
    "#   ...and testing points\n",
    "plt.scatter(Xmoon_test[:, 0], Xmoon_test[:, 1], marker='x', c=y_moon_test, cmap=cm_bright, alpha=0.3)\n",
    "\n",
    "# Define limits/scale of plot axis\n",
    "x_min, x_max = Xmoon[:, 0].min() - .5, Xmoon[:, 0].max() + .5\n",
    "y_min, y_max = Xmoon[:, 1].min() - .5, Xmoon[:, 1].max() + .5\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "# Actually render the plot\n",
    "print()\n",
    "print(\"PLOT OF TRAINING EXAMPLES AND TEST DATASET\")\n",
    "print(\"Datasets: circles=training, light-crosses=test [and red=class_1, blue=class_2]\")\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Model**\n",
    "\n",
    "FIrst we try to build a very simple MLP (input, 2 hidden units with tanh activation, outputs with softmax) and define the needed functions one by one\n",
    "\n",
    "Now we try to define these functions\n",
    "1. initialization\n",
    "2. forward\n",
    "3. loss\n",
    "4. Backward\n",
    "5. optimize\n",
    "\n",
    "Then we will combine all of them as a class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(n_x, n_h,n_y):\n",
    "    \"\"\" \n",
    "    n_x: dimension of input\n",
    "    n_h: dimension of hidden \n",
    "    n_y: dimension of output\n",
    "    \n",
    "    return:\n",
    "    w1(n_h,n_x): weight between input and hidden\n",
    "    b1(n_h): bias 1\n",
    "    w2(n_y,n_h): weight between hidden layer and output layer\n",
    "    b2(n_y): bias 2\n",
    "    \"\"\"\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "#test your function\n",
    "w1, b1, w2,b2 = __init__(2,10,1)\n",
    "print(f'w1:\\n{w1}\\nb1:\\n{b1}\\nw2:\\n{w2}\\nb2:\\n{b2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward**\n",
    "\n",
    "$ h = \\sigma1 (xw_1 + b_1) $, where $w_1$ and $b_1$ are respectively the weight and the bias used to compute the hidden unit from the input.\n",
    "\n",
    "$ y = \\sigma2 (hw_2 + b_2) $, where $w_2$ and $b_2$ are respectively the weight and the bias used to compute the output from the hidden unit.\n",
    "\n",
    "choose $\\sigma_1$ as tanh and $\\sigma_2$ as sigmoid\n",
    "\n",
    "For the computational speed, we will output internal values for backward function. Note: we don't need to do it once we combine all the functions in to one class (they will share values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X,w1,b1,w2,b2):\n",
    "    # TODO: fill in your code\n",
    "    # z1: w1x+b\n",
    "    # A1: tanh(z1)\n",
    "    # z2: w2A1+b2\n",
    "    # A2: sigmoid(z2)\n",
    "    \n",
    "    return 0, 0 #output A2, A1\n",
    "\n",
    "\n",
    "# A little test\n",
    "(output,A1) = forward(Xmoon_train[1, :],w1,b1,w2,b2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will consider as a loss function the squared error defined as :\n",
    "\n",
    "$ L = \\frac{1}{2} (y-y_T)^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(Y,y):\n",
    "    #TODO\n",
    "    loss = (y-Y)**2/2 #revise it to the correct one\n",
    "    return loss \n",
    "\n",
    "# A little test\n",
    "print(loss(1,0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation**\n",
    "\n",
    "we can express the gradient of L with respect to the weights (or biases) as:\n",
    "\n",
    "$ \n",
    "\\begin{equation} \n",
    "    \\begin{split}\n",
    "        \\frac{\\partial L}{\\partial w_2} & = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial w_2}\\\\\n",
    "                                        & = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z_2} \\frac{\\partial z_2}{\\partial w_2} \\\\\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "\n",
    "Same goes for the weight $ w_1 $:\n",
    "\n",
    "$ \n",
    "\\begin{equation} \n",
    "    \\begin{split}\n",
    "        \\frac{\\partial L}{\\partial w_1} & = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial w_1}\\\\\n",
    "                                        & = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z_2} \\frac{\\partial z_2}{\\partial w_1} \\\\\n",
    "                                        & = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z_2} \\frac{\\partial z_2}{\\partial h} \\frac{\\partial h}{\\partial w_1} \\\\\n",
    "                                        & = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z_2} \\frac{\\partial z_2}{\\partial h} \\frac{\\partial h}{\\partial z_1} \\frac{\\partial z_1}{\\partial w_1}\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "One very important thing to notice here is that the evaluation of the gradient $\\frac{\\partial L}{\\partial w_1}$ can reuse some of the calculations perfomed during the evaluation of the gradient $\\frac{\\partial L}{\\partial w_2}$. It is even clearer if we evaluate the gradient $\\frac{\\partial L}{\\partial b_1}$:\n",
    "\n",
    "$ \n",
    "\\begin{equation} \n",
    "    \\begin{split}\n",
    "        \\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z_2} \\frac{\\partial z_2}{\\partial h} \\frac{\\partial h}{\\partial z_1} \\frac{\\partial z_1}{\\partial b_1}\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "We see that the first four term on the righ hand of the equation are the same than the one from $\\frac{\\partial L}{\\partial w_1}$.\n",
    "\n",
    "As you can see in the equations above, we calculate the gradient starting from the end of the computational graph, and proceed backward to get the gradient of the loss with respect to the weights (or the biases). This backward evaluation gives its name to the algoritm: backpropagation\n",
    "\n",
    "In pratice, one iteration of gradient descent would now require one forward pass, and only one pass in the reverse direction computing all the partial derivatives starting from the output node. It is therefore way more efficient than the previous approaches. In the original paper about backpropagation published in 1986 [4] , the authors (among which Geoffrey Hinton) used for the first time backpropagation to allow internal hidden units to learn features of the task domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions you can use: np.dot, np.sum\n",
    "\n",
    "def backward(X,Y,A1):\n",
    "    \"\"\" Back-progagate gradient of the loss\n",
    "    X: input\n",
    "    Y: target\n",
    "    A1: output of hidden neuron\"\"\"\n",
    "    m = X.shape[0]\n",
    "    #TODO: update these values\n",
    "    dZ2 = 0\n",
    "    dW2 = 0\n",
    "    db2 = 0\n",
    "    dZ1 = 0\n",
    "    dW1 = 0\n",
    "    db1 = 0\n",
    "    return (dZ2, dW2,db2,dZ1,dW1,db1)\n",
    "\n",
    "# Little test\n",
    "(dZ2, dW2,db2,dZ1,dW1,db1) = backward(Xmoon_train[1, :],y_moon_train[1],A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we combines they into a class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neural Network\n",
    "class Neural_Network:\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        # Network dimensions\n",
    "        self.n_x = n_in\n",
    "        self.n_h = n_hidden\n",
    "        self.n_y = n_out\n",
    "        \n",
    "        # Parameters initialization\n",
    "        self.W1 = np.random.randn(self.n_h, self.n_x) * 0.01\n",
    "        self.b1 = np.zeros((self.n_h, 1))\n",
    "        self.W2 = np.random.randn(self.n_y, self.n_h) * 0.01\n",
    "        self.b2 = np.zeros((self.n_y, 1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\" Forward computation \"\"\"\n",
    "\n",
    "    \n",
    "    def back_prop(self,  X, Y):\n",
    "        \"\"\" Back-progagate gradient of the loss \"\"\"\n",
    "        m = X.shape[0]\n",
    "        # TODO\n",
    "\n",
    "    def train(self, X, Y, epochs, learning_rate=1.2):\n",
    "        \"\"\" Complete process of learning, alternates forward pass,\n",
    "            backward pass and parameters update \"\"\"\n",
    "        m = X.shape[0]\n",
    "        for e in range(epochs):\n",
    "            # TODO\n",
    "            if e % 1000 == 0:\n",
    "                print(\"Loss \",  e, \" = \", loss)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Compute predictions with just a forward pass \"\"\"\n",
    "        self.forward(X)\n",
    "        return np.round(self.A2).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we create a class of it, train and see the prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Neural_Network(2, 10, 1)\n",
    "nn.train(Xmoon_train, y_moon_train, 5000, 1.2)\n",
    "\n",
    "#show_predictions(nn, X, Y, \"Neural Network\")\n",
    "\n",
    "nn_predictions = nn.predict(Xmoon_train)\n",
    "print(\"Neural Network accuracy on training: \", np.sum(nn_predictions == y_moon_train) / y_moon_train.shape[0])\n",
    "\n",
    "nn_predictions = nn.predict(Xmoon_test)\n",
    "print(\"Neural Network accuracy on testing: \", np.sum(nn_predictions == y_moon_test) / y_moon_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building, training and evaluating it by using Scikit-learn class\n",
    "\n",
    "Please  read the [*MLPClassifier documentation*](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifierMLPClassifier); to understand all parameters of the constructor.\n",
    "You can then begin by running the code block below, in which an initial set of hyper-parameter values has been chosen.\n",
    "**YOU MAY NEED TO CHANGE AT LEAST THE NUMBER OF HIDDEN NEURONS (and probably other hyper-parameters) IN ORDER TO BE ABLE TO LEARN A CORRECT CLASSIFIER**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# Create, fit and evaluate a MLP neural network classifier\n",
    "#########################################################\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Create the MLP (with specific values for hyper-parameters)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1, ), activation='tanh', solver='sgd', \n",
    "                    alpha=0.0000001, batch_size=4, learning_rate='constant', learning_rate_init=0.005, \n",
    "                    power_t=0.5, max_iter=9, shuffle=True, random_state=11, tol=0.00001, \n",
    "                    verbose=True, warm_start=False, momentum=0.8, nesterovs_momentum=True, \n",
    "                    early_stopping=False, validation_fraction=0.2, \n",
    "                    beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "print(mlp)\n",
    "# NB about syntax for hidden layers: hidden_layer_sizes=(H1, ) means ONE hidden layer containing H1 neurons,\n",
    "#   while hidden_layer_sizes=(H1,H2, ) would mean TWO hidden layers of respective sizes H1 and H2\n",
    "# NB about iteration: max_iter specifies a number of EPOCHS (= going through all training examples)\n",
    "\n",
    "# Train the MLP classifier on the training dataset\n",
    "mlp.fit(Xmoon_train, Ymoon_train_OneHot)\n",
    "print()\n",
    "\n",
    "# Plot the LEARNING CURVE\n",
    "plt.title(\"Evolution of TRAINING ERROR during training\")\n",
    "plt.xlabel(\"Iterations (epochs)\")\n",
    "plt.ylabel(\"TRAINING ERROR\")\n",
    "plt.plot(mlp.loss_curve_)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate acuracy on TEST data\n",
    "score = mlp.score(Xmoon_test,Ymoon_test_OneHot)\n",
    "print(\"Acuracy (on test set) = \", score)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below, we visualize the learnt boundary between classes in (2D) input space ##\n",
    "\n",
    "**THIS SHOULD HELP YOU UNDERSTAND WHAT HAPPENS IF THERE ARE NOT ENOUGH HIDDEN NEURONS**\n",
    "\n",
    "Optional: add code that visualises on the same plot the straight lines corresponding to each hidden neuron (you will need to dig into MLPClassifier documentation to find the 2 input weights and the bias of each hidden neuron). YOU SHOULD NOTICE THAT THE CLASSIFICATION BOUNDARY IS SOME INTERPOLATION BETWEEN THOSE STRAIGHT LINES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "#   point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "h = .02  # Step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "# Compute class probabilities for each mesh point\n",
    "Z = mlp.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(Xmoon_train[:, 0], Xmoon_train[:, 1], c=y_moon_train, cmap=cm_bright)\n",
    "# and testing points\n",
    "plt.scatter(Xmoon_test[:, 0], Xmoon_test[:, 1], marker='x', c=y_moon_test, cmap=cm_bright, alpha=0.3)\n",
    "\n",
    "# Axis ranges \n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "# Print acuracy on plot\n",
    "plt.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "\n",
    "# Actually plot\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, **check, by changing MLPClassifier parameters above and then rerunning training+eval+plots, the impact of main learning hyper-parameters:**\n",
    "- **number of neurons on hidden layer**: if very small, the classification boundary shall be too simple; if too large, overfitting might occur more easily. \n",
    "    **NB: generally, only ONE hidden layer is sufficient (cf. Cybenko theorem)**; *using more than one may require using ReLU as activation function, to avoid gradient \"vanishing\".*\n",
    "- **activation functions**\n",
    "- **number of iterations**: if too small, the training does not finish to converge; if too large, overfitting may occur. \n",
    "   **NB: it is therefore usually better to use \"early_stopping\" with quite large max_iter, so that the actual number of iterations shall adapt by STOPPING WHEN VALIDATION ERROR STOPS DECREASING**\n",
    "- **solver** (the best choice is generally 'adam'; for more details, see https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py) \n",
    "- **learning_rate and momentum: the *initial learning rate* ALWAYS impacts training outcome a lot** (too small may stuck training in bad local minimum; too large can induce strong error fluctuations and possibly no convergence)\n",
    "- **impact of L2 weight regularization term (alpha)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### **Finally, use grid-search and cross-validation to find an optimized set of learning hyper-parameters (see code below).**\n",
    "\n",
    "**Because the values of learning hyper-parameters can DRASTICALLY change the outcome of training, it is ESSENTIAL THAT YOU ALWAYS TRY TO FIND OPTIMIZED VALUES FOR THE ALGORITHM HYPER-PARAMETERS. And this ABSOLUTELY NEEDS TO BE DONE USING \"VALIDATION\", either with a validation set separate from the training set, or using cross-validation. CROSS-VALIDATION is the MOST ROBUST WAY OF FINDING OPTIMIZED HYPER-PARAMETRS VALUES, and the GridSearchCV function of SciKit-Learn makes this rather straightforward.**\n",
    "\n",
    "**WARNING:** GridSearchCV launches many successive training sessions, so **can be rather long to execute if you compare too many combinations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "param_grid = [\n",
    "  {'hidden_layer_sizes': [(5,), (10,), (20,), (40,)], \n",
    "   'learning_rate_init':[0.003, 0.01, 0.03, 0.1],\n",
    "   'alpha': [0.00001, 0.0001, 0.001]}\n",
    " ]\n",
    "print(param_grid)\n",
    "\n",
    "# Cross-validation grid-search (for finding best possible accuracy)\n",
    "clf = GridSearchCV( MLPClassifier(activation='tanh', alpha=1e-07, batch_size=4, beta_1=0.9,\n",
    "                                  beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
    "                                  hidden_layer_sizes=(10,), learning_rate='constant',\n",
    "                                  learning_rate_init=0.005, max_iter=500, momentum=0.8,\n",
    "                                  nesterovs_momentum=True, power_t=0.5, random_state=11, shuffle=True,\n",
    "                                  solver='adam', tol=1e-05, validation_fraction=0.3, verbose=False,\n",
    "                                  warm_start=False), \n",
    "                   param_grid, cv=3, scoring='accuracy') \n",
    "# NOTE THAT YOU CAN USE OTHER VALUE FOR cv (# of folds) and OTHER SCORING CRITERIA OTHER THAN 'accuracy'\n",
    "    \n",
    "clf.fit(Xmoon_train, Ymoon_train_OneHot)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "           % (mean, std * 2, params))\n",
    "print()\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n",
    "y_true, y_pred = Ymoon_test_OneHot, clf.predict(Xmoon_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the code is revised from Prof. Fabien Moutarde\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HPC_IA_env",
   "language": "python",
   "name": "hpc_ia_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
