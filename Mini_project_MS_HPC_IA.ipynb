{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mini project MS HPC-IA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y6ehIVfhZ7b"
      },
      "source": [
        "# Deep-Learning mini-projects for the MS HPC-IA \r\n",
        "\r\n",
        "Pr. HsiuWen(Kelly) Chang Joly (Associate professor at the [center for Robotics](http://caor.mines-paristech.fr/), [MINES ParisTech](http://www.mines-paristech.fr/), [PSL Universit√© Paris](https://www.univ-psl.fr/)\r\n",
        "\r\n",
        "You have to choose 1 of the 5 mini-projects topics below :\r\n",
        "\r\n",
        "## Transfer-learning\r\n",
        "\r\n",
        "Dataset: [102-flowers](https://cloud.mines-paristech.fr/index.php/s/ekN3uXoJSrbbSD8)\r\n",
        "\r\n",
        "In order to obtain a convNet that is able to [recognize\r\n",
        "types of flowers](https://MiniProj_MS-HPC-IA/flowers_notebook.html)\r\n",
        "[NB: example code illustrating how to load pre-trained convNets, and\r\n",
        "then modify it; freeze some layers for transfer Learning are\r\n",
        "avilable on web [page](https://keras.io/applications/) Applications - Keras\r\n",
        "Documentation\r\n",
        "\r\n",
        "## LSTM topic\r\n",
        "\r\n",
        "Dataset: [DHG/SHREC2017](https://cloud.mines-paristech.fr/index.php/s/OzF1Hu9ZyXSsfqN)\r\n",
        "Use this dataset for [recognition of hand gestures](MiniProj_MS-HPC-IA/handGesturesDHG_notebook.html) (represented by joints trajectories):compare [multiples 1D temporal convolutions temporelles 1D](MiniProj_MS-HPC-IA/devineau_2018_deep_learning_hand_gesture_pytorch_model_quickstart.html) and Recurrent LSTM Neural Network\r\n",
        "\r\n",
        "## Training a semantic segmentation covnet\r\n",
        "\r\n",
        "[description](MiniProj_MS-HPC-IA/semantic_segmentation_FCN_DeepLab_pyTorch.html)\r\n",
        "Dataset: [simulator](https://cloud.mines-paristech.fr/index.php/s/Z7M1FuTOe4uf7pl) (produced by a simulator) containing images and\r\n",
        "associated semantic segmentation ground truth (=class target values for\r\n",
        "each pixel)\r\n",
        "\r\n",
        "## Reinforement learning project\r\n",
        "\r\n",
        "As described in our session 4 notebook, implement [DQN](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) on the [Enduro-v0](https://gym.openai.com/envs/Enduro-v0/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysRG_Z2-kxhJ"
      },
      "source": [
        "In all cases, the result of your mini-project work should be\r\n",
        "provided as a NOTEBOOK containing all the developped *code with\r\n",
        "execution results* AND TEXT CELLS CONTAINING&nbsp; EXPLANATIONS +\r\n",
        "ANALYSIS/SUMMARY OF OBTAINED RESULTS. This notebook should be sent by\r\n",
        "e-mail at hsiu-wen.chang_joly@mines-paristech.fr\r\n",
        "(PS: in case the notebook&nbsp;would too big for e-mail attachment,\r\n",
        "just put it somewhere on the cloud and provide the download link in the\r\n",
        "e-mail)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXXaZhA0hUTM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}