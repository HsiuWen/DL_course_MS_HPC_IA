{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8y6ehIVfhZ7b"
   },
   "source": [
    "# Deep-Learning mini-projects for the MS HPC-IA \n",
    "\n",
    "Pr. HsiuWen(Kelly) Chang Joly (Associate professor at the [center for Robotics](http://caor.mines-paristech.fr/), [MINES ParisTech](http://www.mines-paristech.fr/), [PSL Universit√© Paris](https://www.univ-psl.fr/)\n",
    "\n",
    "You have to choose 1 of the 5 mini-projects topics below :\n",
    "\n",
    "## Transfer-learning\n",
    "\n",
    "Dataset: [102-flowers](https://cloud.mines-paristech.fr/index.php/s/ekN3uXoJSrbbSD8)\n",
    "\n",
    "In order to obtain a convNet that is able to [recognize\n",
    "types of flowers](MiniProj_MS-HPC-IA/flowers_notebook.html)\n",
    "[NB: example code illustrating how to load pre-trained convNets, and\n",
    "then modify it; freeze some layers for transfer Learning are\n",
    "avilable on web [page](https://keras.io/applications/) Applications - Keras\n",
    "Documentation\n",
    "\n",
    "## LSTM topic\n",
    "\n",
    "Dataset: [DHG/SHREC2017](https://cloud.mines-paristech.fr/index.php/s/OzF1Hu9ZyXSsfqN)\n",
    "\n",
    "In this downloaded file, you will have sequence of 2 joints trajectories recorded by depth camera. You should start with the notebook file [recognition of hand gestures](MiniProj_MS-HPC-IA/handGesturesDHG_notebook.html) (represented by joints trajectories) in order to see the figures and understand the very basic way to classify these depth trajectories into 14 (or you can do 28) categories of hand gesture (grab, swift left, tap, expand, pinch, ...)\n",
    "\n",
    "The task in this topic is to compare [multiples 1D temporal convolutions temporelles 1D](MiniProj_MS-HPC-IA/devineau_2018_deep_learning_hand_gesture_pytorch_model_quickstart.html) and Recurrent LSTM Neural Network. The previous model is finished in the notebook and what you need to do is add a new class that run LSTM on the same dataset in order to compare the different performance. \n",
    "\n",
    "## Training a semantic segmentation covnet\n",
    "\n",
    "[description](MiniProj_MS-HPC-IA/semantic_segmentation_FCN_DeepLab_pyTorch.html)\n",
    "Dataset: [simulator](https://cloud.mines-paristech.fr/index.php/s/Z7M1FuTOe4uf7pl) (produced by a simulator) containing images and\n",
    "associated semantic segmentation ground truth (=class target values for\n",
    "each pixel)\n",
    "\n",
    "## Reinforement learning project\n",
    "\n",
    "As described in our [session 4 notebook](Lb4_reinforcement_learning_Q.ipynb), implement Deep Q-learning neural network [DQN](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) on the [Enduro-v0](https://gym.openai.com/envs/Enduro-v0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysRG_Z2-kxhJ"
   },
   "source": [
    "In all cases, the result of your mini-project work should be\n",
    "provided as a NOTEBOOK containing all the developped *code with\n",
    "execution results* AND TEXT CELLS CONTAINING&nbsp; EXPLANATIONS +\n",
    "ANALYSIS/SUMMARY OF OBTAINED RESULTS. This notebook should be sent by\n",
    "e-mail at hsiu-wen.chang_joly@mines-paristech.fr\n",
    "(PS: in case the notebook&nbsp;would too big for e-mail attachment,\n",
    "just put it somewhere on the cloud and provide the download link in the\n",
    "e-mail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXXaZhA0hUTM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Mini project MS HPC-IA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
