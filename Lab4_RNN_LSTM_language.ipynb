{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lb3_RNN_LSTM_language.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7WBO0STnTG2"
      },
      "source": [
        "# Lab3 Implementation of simple RNN and LSTM \r\n",
        "\r\n",
        "This notebook has been prepared by Hsiu-Wen Chang from MINES ParisTech\r\n",
        "Shall you have any problem, send me [email](hsiu-wen.chang_joly@mines-paristech.fr)\r\n",
        "\r\n",
        "In this lab, we are going to practice \r\n",
        "\r\n",
        "1. many-to-one RNN\r\n",
        "2. many-to-one by LSTM\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ohpiJoXn9nT"
      },
      "source": [
        "## 1. Many-to-one RNN: Predict what is the next word\r\n",
        "\r\n",
        "Our design task today is to predict the next word given several words before. For example, when we key in 'I like' the expected answer from the machine should be 'cat'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4A_SAJUkz9T"
      },
      "source": [
        "# Configuration\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.autograd import Variable\r\n",
        "\r\n",
        "dtype = torch.FloatTensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHwH300KolLb"
      },
      "source": [
        "### 1.1 Data preparation\r\n",
        "\r\n",
        "Here are create three sentences and each of them has three words. We are going to feed first two words and let the machine find the final word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URZTfCdrodnO"
      },
      "source": [
        "# Create the input data, you are welcome to add the words you like\r\n",
        "sentences = [ \"i like cat\", \"i love coffee\", \"i hate milk\"]\r\n",
        "\r\n",
        "# Define all the possible words\r\n",
        "word_list = \" \".join(sentences).split()\r\n",
        "\r\n",
        "word_list = list(set(word_list))\r\n",
        "\r\n",
        "# dictionary that chanage the given letter to number. {love: 0, hate:1,...}\r\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\r\n",
        "\r\n",
        "# dictionary that chanage the number to letter. {0: love, 1: hate,...}\r\n",
        "number_dict = {i: w for i, w in enumerate(word_list)}\r\n",
        "\r\n",
        "# number of class(=number of vocab)\r\n",
        "n_class = len(word_dict)\r\n",
        "\r\n",
        "print(word_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENif1UQqE5Vi"
      },
      "source": [
        "### 1.2 Data preprocessing\r\n",
        "\r\n",
        "Define batch function to let machine know how he should use it during training.\r\n",
        "Here we give all the data we have for simplication. But in real case, you should not do it.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lSpSxU2D5r4"
      },
      "source": [
        "# Function to encode the sentence into a vector \r\n",
        "def make_batch(sentences):\r\n",
        "    input_batch = []\r\n",
        "    target_batch = []\r\n",
        "\r\n",
        "    for sen in sentences:\r\n",
        "        word = sen.split()\r\n",
        "        input = [word_dict[n] for n in word[:-1]]\r\n",
        "        target = word_dict[word[-1]]\r\n",
        "\r\n",
        "        input_batch.append(np.eye(n_class)[input])\r\n",
        "        target_batch.append(target)\r\n",
        "\r\n",
        "    return input_batch, target_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ5-1MR4FFiF"
      },
      "source": [
        "# to Torch.Tensor\r\n",
        "input_batch, target_batch = make_batch(sentences)\r\n",
        "input_batch = Variable(torch.Tensor(input_batch))\r\n",
        "target_batch = Variable(torch.LongTensor(target_batch))\r\n",
        "\r\n",
        "print('Dimension of input_patch:', input_batch.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ80UekGDmtw"
      },
      "source": [
        "### 1.3 Network\r\n",
        "\r\n",
        "Torch.nn provide a function call nn.RNN which is a multi-layer Elman RNN with $tanh$ or $ReLU$ (controlled by nonlinearity parameter) to an input sequence.\r\n",
        "\r\n",
        "The equation to compute the hidden state is $$h_t=tanh(W_{ih}x_t+b_{ih}+w_{hh}h_{t-1}+b_{hh}) $$\r\n",
        "\r\n",
        "Further information about how you can use it, check this [link](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMk0ffFHGc8v"
      },
      "source": [
        "class TextRNN(nn.Module):\r\n",
        "    def __init__(self,n_class=7, n_hidden=5):\r\n",
        "        super(TextRNN, self).__init__()\r\n",
        "\r\n",
        "        self.rnn = nn.RNN(input_size=n_class, hidden_size=n_hidden)\r\n",
        "        self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\r\n",
        "        self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\r\n",
        "\r\n",
        "    def forward(self, hidden, X):\r\n",
        "        X = X.transpose(0, 1) # X : [n_step, batch_size, n_class]\r\n",
        "        outputs, hidden = self.rnn(X, hidden)\r\n",
        "        # outputs : [n_step, batch_size, num_directions(=1) * n_hidden]\r\n",
        "        # hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\r\n",
        "        outputs = outputs[-1] # [batch_size, num_directions(=1) * n_hidden]\r\n",
        "        model = torch.mm(outputs, self.W) + self.b # model : [batch_size, n_class]\r\n",
        "        return model\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKdx4ZGduZ19"
      },
      "source": [
        "# Paramters for the network\r\n",
        "batch_size = len(sentences)\r\n",
        "n_step = 2 # number of cells(= number of Step)\r\n",
        "n_hidden = 5 # number of hidden units in one cell\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGqRO_R0P5aD"
      },
      "source": [
        "model = TextRNN(n_class, n_hidden)\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZACZzfaVQaZl"
      },
      "source": [
        "Lets see how this model looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyC3Un5kP8vN"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usnLJmJhSyV9"
      },
      "source": [
        "### 1.4 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ2sYct1QJqF"
      },
      "source": [
        "# Training\r\n",
        "for epoch in range(5000):\r\n",
        "    # Reset the gradient buffer \r\n",
        "    optimizer.zero_grad() \r\n",
        "\r\n",
        "    # hidden : [num_layers * num_directions, batch, hidden_size]\r\n",
        "    hidden = Variable(torch.zeros(1, batch_size, n_hidden))\r\n",
        "    # input_batch : [batch_size, n_step, n_class]\r\n",
        "    output = model(hidden, input_batch)\r\n",
        "\r\n",
        "    # output : [batch_size, n_class], target_batch : [batch_size] (LongTensor, not one-hot)\r\n",
        "    loss = criterion(output, target_batch)\r\n",
        "    if (epoch + 1) % 1000 == 0:\r\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\r\n",
        "\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_N2M5rSTcNj"
      },
      "source": [
        "### 1.5 Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9StYmSPjTP7N"
      },
      "source": [
        "# Predict\r\n",
        "# Initial hidden state 0\r\n",
        "hidden = Variable(torch.zeros(1, batch_size, n_hidden))\r\n",
        "\r\n",
        "print('Raw output of this model:\\n',model(hidden, input_batch))\r\n",
        "\r\n",
        "predict = model(hidden, input_batch).data.max(1, keepdim=True)[1]\r\n",
        "print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29mr61tFVEmX"
      },
      "source": [
        "### Task 1: create the sentences by yourself and predict the final word by using the same model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSBZJqpRTiY7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjsQaSOkVk8i"
      },
      "source": [
        "##2. Many-to-one LSTM: Predict what is the next letter\r\n",
        "\r\n",
        "In this task, we will give our network to predict the final letter for us uisng LSTM. For example, if we key in 'lov' then the machine should give us 'e'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTJ8yumPag7y"
      },
      "source": [
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo24zXN1d131"
      },
      "source": [
        "### 2.1 Data preparation and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1BkuKk-db_c"
      },
      "source": [
        "# we need define all the possible letters\r\n",
        "char_arr = [c for c in 'abcdefghijklmnopqrstuvwxyz']\r\n",
        "\r\n",
        "#word dictionary that can use to get the corresponding encoded number\r\n",
        "word_dict = {n: i for i, n in enumerate(char_arr)}\r\n",
        "\r\n",
        "# number dictionary that can be used to get the corresponding letter\r\n",
        "number_dict = {i: w for i, w in enumerate(char_arr)}\r\n",
        "\r\n",
        "n_class = len(word_dict) # number of class(=number of vocab)\r\n",
        "\r\n",
        "seq_data = ['make', 'need', 'coal', 'word', 'love', 'hate', 'live', 'home', 'hash', 'star']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTNi04tpeCNJ"
      },
      "source": [
        "\r\n",
        "def make_batch(seq_data):\r\n",
        "    input_batch, target_batch = [], []\r\n",
        "\r\n",
        "    for seq in seq_data:\r\n",
        "        input = [word_dict[n] for n in seq[:-1]] # 'm', 'a' , 'k' is input\r\n",
        "        target = word_dict[seq[-1]] # 'e' is target\r\n",
        "        input_batch.append(np.eye(n_class)[input])\r\n",
        "        target_batch.append(target)\r\n",
        "\r\n",
        "    return Variable(torch.Tensor(input_batch)), Variable(torch.LongTensor(target_batch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyze2ep0eOWj"
      },
      "source": [
        "### 2.2 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RV0SyyCeKaE"
      },
      "source": [
        "# TextLSTM Parameters\r\n",
        "n_step = 3\r\n",
        "n_hidden = 128\r\n",
        "\r\n",
        "class TextLSTM(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(TextLSTM, self).__init__()\r\n",
        "\r\n",
        "        self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden)\r\n",
        "        self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\r\n",
        "        self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\r\n",
        "\r\n",
        "    def forward(self, X):\r\n",
        "        input = X.transpose(0, 1)  # X : [n_step, batch_size, n_class]\r\n",
        "\r\n",
        "        hidden_state = Variable(torch.zeros(1, len(X), n_hidden))   # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\r\n",
        "        cell_state = Variable(torch.zeros(1, len(X), n_hidden))     # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\r\n",
        "\r\n",
        "        outputs, (_, _) = self.lstm(input, (hidden_state, cell_state))\r\n",
        "        outputs = outputs[-1]  # [batch_size, n_hidden]\r\n",
        "        model = torch.mm(outputs, self.W) + self.b  # model : [batch_size, n_class]\r\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoQyxb9RehCj"
      },
      "source": [
        "### 2.3 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9wn3ZP-eonH"
      },
      "source": [
        "input_batch, target_batch = make_batch(seq_data)\r\n",
        "\r\n",
        "model = TextLSTM()\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\r\n",
        "\r\n",
        "output = model(input_batch)\r\n",
        "\r\n",
        "# Training\r\n",
        "for epoch in range(1000):\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    output = model(input_batch)\r\n",
        "    loss = criterion(output, target_batch)\r\n",
        "    if (epoch + 1) % 100 == 0:\r\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\r\n",
        "\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHlCD3aiez7d"
      },
      "source": [
        "### 2.4 Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj_ODzDxev9Q"
      },
      "source": [
        "inputs = [sen[:3] for sen in seq_data]\r\n",
        "\r\n",
        "predict = model(input_batch).data.max(1, keepdim=True)[1]\r\n",
        "print(inputs, '->', [number_dict[n.item()] for n in predict.squeeze()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8df7rkUfXfR"
      },
      "source": [
        "### Task 2: \r\n",
        "\r\n",
        "1. Use whatever way you like, add more than 100 vocabulary and reuse the code to do the save task\r\n",
        "\r\n",
        "1. modify the model to make it predict one word each \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-ZJhmSfe5-X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaLAB5CV-biG"
      },
      "source": [
        "## Conclusion\r\n",
        "\r\n",
        "You should think about the problem when we have much bigger vocabulary that using dict to enumerate the words will make it very inefficient.\r\n",
        "\"Embedding\" and \"Tokenizer\" are the two soltuions available in [Keras](https://keras.io/examples/nlp/). You should take a look at this document \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5Gq-iEA-9wC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}