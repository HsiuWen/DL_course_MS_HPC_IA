{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab4_RNN_LSTM_language.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNcjO9odI67yqCTVsVW6In"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7WBO0STnTG2"
      },
      "source": [
        "# Lab3 Implementation of simple RNN and LSTM \n",
        "\n",
        "This notebook has been prepared by Hsiu-Wen Chang from MINES ParisTech\n",
        "Shall you have any problem, send me [email](hsiu-wen.chang_joly@mines-paristech.fr)\n",
        "\n",
        "In this lab, we are going to practice \n",
        "\n",
        "1. many-to-one by RNN: given several words, predict the next word\n",
        "2. many-to-one by LSTM: given several letters, predict the final letter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ohpiJoXn9nT"
      },
      "source": [
        "## 1. Many-to-one by RNN: Predict what is the next word\n",
        "\n",
        "Our task today is to predict the next word by given several words before. For example, we expect to have answer to be 'cat' when user key in 'I like'."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FM9dPi4dnXQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4A_SAJUkz9T"
      },
      "source": [
        "# Configuration\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "dtype = torch.FloatTensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHwH300KolLb"
      },
      "source": [
        "### 1.1 Data preparation\n",
        "\n",
        "Here are three sentences and each of them has three words. We are going to use it as training sample. The design is to feed first two words and let the machine find the final word. However, the computer can't do mathematic operations on characters. Therefore, the first step is to encode the input to digital numbers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URZTfCdrodnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bfe2f84-20f2-4804-ad00-95c7f32ea555"
      },
      "source": [
        "# Create the input data, you are welcome to add the words you like\n",
        "sentences = [ \"i like cat\", \"i love coffee\", \"i hate milk\"]\n",
        "\n",
        "# Define all the possible words\n",
        "word_list = \" \".join(sentences).split()\n",
        "\n",
        "word_list = list(set(word_list))\n",
        "\n",
        "# dictionary that chanage the given word to number. {love: 0, hate:1,...}\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "\n",
        "# dictionary that chanage the number to word. {0: love, 1: hate,...}\n",
        "number_dict = {i: w for i, w in enumerate(word_list)}\n",
        "\n",
        "# number of class(=number of vocab)\n",
        "n_class = len(word_dict)\n",
        "\n",
        "print(word_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'milk': 0, 'cat': 1, 'like': 2, 'love': 3, 'coffee': 4, 'hate': 5, 'i': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENif1UQqE5Vi"
      },
      "source": [
        "### 1.2 Data preprocessing\n",
        "\n",
        "Define batch function to let machine know how he should use it during training.\n",
        "Here we give all the data we have for simplication. But in real case, you should not do it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lSpSxU2D5r4"
      },
      "source": [
        "# Function to encode the sentence into a vector \n",
        "def make_batch(sentences):\n",
        "    input_batch = []\n",
        "    target_batch = []\n",
        "\n",
        "    for sen in sentences:\n",
        "        word = sen.split()\n",
        "        input = [word_dict[n] for n in word[:-1]]\n",
        "        target = word_dict[word[-1]]\n",
        "\n",
        "        input_batch.append(np.eye(n_class)[input])\n",
        "        target_batch.append(target)\n",
        "\n",
        "    return input_batch, target_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ5-1MR4FFiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5669b385-d1a8-408f-a4f9-58aecc9bdc92"
      },
      "source": [
        "# to Torch.Tensor\n",
        "input_batch, target_batch = make_batch(sentences)\n",
        "input_batch = Variable(torch.Tensor(input_batch))\n",
        "target_batch = Variable(torch.LongTensor(target_batch))\n",
        "\n",
        "print('Dimension of input_patch:', input_batch.shape)\n",
        "print(input_batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimension of input_patch: torch.Size([3, 2, 7])\n",
            "tensor([[[0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 1., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 1., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 1., 0.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ80UekGDmtw"
      },
      "source": [
        "### 1.3 Network\n",
        "\n",
        "Torch.nn provide a function call nn.RNN which is a multi-layer Elman RNN with $tanh$ or $ReLU$ (controlled by nonlinearity parameter) to an input sequence.\n",
        "\n",
        "The equation to compute the hidden state is $$h_t=tanh(W_{ih}x_t+b_{ih}+w_{hh}h_{t-1}+b_{hh}) $$\n",
        "\n",
        "Further information about how you can use it, check this [link](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMk0ffFHGc8v"
      },
      "source": [
        "class TextRNN(nn.Module):\n",
        "    def __init__(self,n_class=7, n_hidden=5):\n",
        "        super(TextRNN, self).__init__()\n",
        "\n",
        "        self.rnn = nn.RNN(input_size=n_class, hidden_size=n_hidden)\n",
        "        self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\n",
        "        self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n",
        "\n",
        "    def forward(self, hidden, X):\n",
        "        X = X.transpose(0, 1) # X : [n_step, batch_size, n_class]\n",
        "        outputs, hidden = self.rnn(X, hidden)\n",
        "        # outputs : [n_step, batch_size, num_directions(=1) * n_hidden]\n",
        "        # hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "        outputs = outputs[-1] # [batch_size, num_directions(=1) * n_hidden]\n",
        "        model = torch.mm(outputs, self.W) + self.b # model : [batch_size, n_class]\n",
        "        return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKdx4ZGduZ19"
      },
      "source": [
        "# Paramters for the network\n",
        "batch_size = len(sentences)\n",
        "n_step = 2 # number of cells(= number of Step)\n",
        "n_hidden = 5 # number of hidden units in one cell\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGqRO_R0P5aD"
      },
      "source": [
        "model = TextRNN(n_class, n_hidden)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZACZzfaVQaZl"
      },
      "source": [
        "Lets see how this model looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyC3Un5kP8vN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddbdbb64-6d7d-45fb-89ea-150d5f34ff39"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TextRNN(\n",
            "  (rnn): RNN(7, 5)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usnLJmJhSyV9"
      },
      "source": [
        "### 1.4 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ2sYct1QJqF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0a6f249-8a97-4494-d9d5-f3faf6b4eff4"
      },
      "source": [
        "# Training\n",
        "for epoch in range(5000):\n",
        "    # Reset the gradient buffer \n",
        "    optimizer.zero_grad() \n",
        "\n",
        "    # hidden : [num_layers * num_directions, batch, hidden_size]\n",
        "    hidden = Variable(torch.zeros(1, batch_size, n_hidden))\n",
        "    # input_batch : [batch_size, n_step, n_class]\n",
        "    output = model(hidden, input_batch)\n",
        "\n",
        "    # output : [batch_size, n_class], target_batch : [batch_size] (LongTensor, not one-hot)\n",
        "    loss = criterion(output, target_batch)\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1000 cost = 0.125204\n",
            "Epoch: 2000 cost = 0.022907\n",
            "Epoch: 3000 cost = 0.008536\n",
            "Epoch: 4000 cost = 0.004017\n",
            "Epoch: 5000 cost = 0.002101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_N2M5rSTcNj"
      },
      "source": [
        "### 1.5 Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9StYmSPjTP7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1454837b-05bd-44f3-a958-f42626d05eaa"
      },
      "source": [
        "# Predict\n",
        "# Initial hidden state 0\n",
        "hidden = Variable(torch.zeros(1, batch_size, n_hidden))\n",
        "\n",
        "print('Raw output of this model:\\n',model(hidden, input_batch))\n",
        "\n",
        "predict = model(hidden, input_batch).data.max(1, keepdim=True)[1]\n",
        "print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])#"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw output of this model:\n",
            " tensor([[-3.6638,  7.1956, -0.5065, -4.1369,  0.4596, -1.5824, -1.6707],\n",
            "        [ 0.7433,  0.8901, -3.2157, -5.8117,  7.6240, -2.9111, -4.4744],\n",
            "        [ 6.6748, -3.9425, -0.9100, -4.3538, -0.0419, -2.0215, -2.1706]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "[['i', 'like'], ['i', 'love'], ['i', 'hate']] -> ['cat', 'coffee', 'milk']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29mr61tFVEmX"
      },
      "source": [
        "### Task 1: create the sentences by yourself and predict the final word by using the same model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSBZJqpRTiY7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjsQaSOkVk8i"
      },
      "source": [
        "##2. Many-to-one LSTM: Predict what is the next letter\n",
        "\n",
        "In this task, we will give our network to predict the final letter for us uisng LSTM. For example, if we key in 'lov' then the machine should give us 'e'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTJ8yumPag7y"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo24zXN1d131"
      },
      "source": [
        "### 2.1 Data preparation and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1BkuKk-db_c"
      },
      "source": [
        "# we need define all the possible letters\n",
        "char_arr = [c for c in 'abcdefghijklmnopqrstuvwxyz']\n",
        "\n",
        "#word dictionary that can use to get the corresponding encoded number\n",
        "word_dict = {n: i for i, n in enumerate(char_arr)}\n",
        "\n",
        "# number dictionary that can be used to get the corresponding letter\n",
        "number_dict = {i: w for i, w in enumerate(char_arr)}\n",
        "\n",
        "n_class = len(word_dict) # number of class(=number of vocab)\n",
        "\n",
        "seq_data = ['make', 'need', 'coal', 'word', 'love', 'hate', 'live', 'home', 'hash', 'star']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTNi04tpeCNJ"
      },
      "source": [
        "def make_batch(seq_data):\n",
        "    input_batch, target_batch = [], []\n",
        "\n",
        "    for seq in seq_data:\n",
        "        input = [word_dict[n] for n in seq[:-1]] # 'm', 'a' , 'k' is input\n",
        "        target = word_dict[seq[-1]] # 'e' is target\n",
        "        input_batch.append(np.eye(n_class)[input])\n",
        "        target_batch.append(target)\n",
        "\n",
        "    return Variable(torch.Tensor(input_batch)), Variable(torch.LongTensor(target_batch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyze2ep0eOWj"
      },
      "source": [
        "### 2.2 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RV0SyyCeKaE"
      },
      "source": [
        "# TextLSTM Parameters\n",
        "n_step = 3\n",
        "n_hidden = 128\n",
        "\n",
        "class TextLSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextLSTM, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden)\n",
        "        self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\n",
        "        self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n",
        "\n",
        "    def forward(self, X):\n",
        "        input = X.transpose(0, 1)  # X : [n_step, batch_size, n_class]\n",
        "\n",
        "        hidden_state = Variable(torch.zeros(1, len(X), n_hidden))   # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "        cell_state = Variable(torch.zeros(1, len(X), n_hidden))     # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "\n",
        "        outputs, (_, _) = self.lstm(input, (hidden_state, cell_state))\n",
        "        outputs = outputs[-1]  # [batch_size, n_hidden]\n",
        "        model = torch.mm(outputs, self.W) + self.b  # model : [batch_size, n_class]\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoQyxb9RehCj"
      },
      "source": [
        "### 2.3 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9wn3ZP-eonH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ebce9b-8633-4990-bd64-a5e8901ad5a3"
      },
      "source": [
        "input_batch, target_batch = make_batch(seq_data)\n",
        "\n",
        "model = TextLSTM()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "output = model(input_batch)\n",
        "\n",
        "# Training\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(input_batch)\n",
        "    loss = criterion(output, target_batch)\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0100 cost = 0.013342\n",
            "Epoch: 0200 cost = 0.003068\n",
            "Epoch: 0300 cost = 0.001384\n",
            "Epoch: 0400 cost = 0.000790\n",
            "Epoch: 0500 cost = 0.000512\n",
            "Epoch: 0600 cost = 0.000360\n",
            "Epoch: 0700 cost = 0.000266\n",
            "Epoch: 0800 cost = 0.000205\n",
            "Epoch: 0900 cost = 0.000163\n",
            "Epoch: 1000 cost = 0.000132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHlCD3aiez7d"
      },
      "source": [
        "### 2.4 Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj_ODzDxev9Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2748de06-871f-48fc-830c-285c0b196213"
      },
      "source": [
        "inputs = [sen[:3] for sen in seq_data]\n",
        "\n",
        "predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
        "print(inputs, '->', [number_dict[n.item()] for n in predict.squeeze()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['mak', 'nee', 'coa', 'wor', 'lov', 'hat', 'liv', 'hom', 'has', 'sta'] -> ['e', 'd', 'l', 'd', 'e', 'e', 'e', 'e', 'h', 'r']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8df7rkUfXfR"
      },
      "source": [
        "### Task 2: \n",
        "\n",
        "1. Use whatever way you like, add more than 100 vocabulary and reuse the code to do the same task\n",
        "1. modify the model to make it predict one word each time\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-ZJhmSfe5-X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaLAB5CV-biG"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "You should think about the problem when we have much bigger vocabulary that using dict to enumerate the words will make it very inefficient.\n",
        "\"Embedding\" and \"Tokenizer\" are the two soltuions available in [Keras](https://keras.io/examples/nlp/). You should take a look at this document \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5Gq-iEA-9wC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(7, 3)"
      ],
      "metadata": {
        "id": "fmPW7hN5DhpL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}