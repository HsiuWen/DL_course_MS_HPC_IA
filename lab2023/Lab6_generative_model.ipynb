{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "advised-return",
   "metadata": {},
   "source": [
    "# Lab 6 Generative models\n",
    "\n",
    "This notebook has been prepared by Hsiu-Wen Chang from MINES ParisTech\n",
    "Shall you have any problem, send me [email](hsiu-wen.chang_joly@mines-paristech.fr)\n",
    "\n",
    "In this practical lesson, we will use Pytorch to learn autoencoder in order to understand the limitations of it. Then the second part introduces variational autoencoder to show how we can genrate new synthetic images from some unknown (but learnable from givien images) distribution P \n",
    "\n",
    "To run this code, you can follow these command to have your virtual environment. If you have a GPU, make sure the your cuda version when you install pytorch\n",
    "* conda create --name lightning_env python=3.7 jupyter\n",
    "* conda activate lightning_env\n",
    "* conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n",
    "* pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f79deaa",
   "metadata": {},
   "source": [
    "## Part 1: Autoencoder\n",
    "\n",
    "Encoder is a process that produces `new features` representation from the `old features`, either by selection or extraction, and decoder the reverse process. When the dimension of the new features (n_d) is relative low compare to the dimension of the old feature (n_e), we call this Dimensionality reduction. This is a popular processfor for data compression such as video, audio, etc. The main purpose of dimensionality reduction is to find the best encoder/decoder pair among a give family. \n",
    "\n",
    "Hereafter, we denote E and D as the families of encoders and decoder, respectively. We define the reconstruction error measures between the input data $x$ and the encoder data $d(e(x))$ as $\\epsilon(x,d(e(x))$. Then we can formulate this problem as:\n",
    "\n",
    "$$(e^*,d^*) = \\underset{(e,d)\\in ExD}{\\operatorname{argmax}}{\\epsilon(x, d(e(x)))}$$\n",
    "\n",
    "The general idea of autoencoders is simple that we try to learn the best encorder $e^*$ and decoder $d^*$ as neural networks using an iterative optimisation processs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "inclusive-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fundamental packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a724e578",
   "metadata": {},
   "source": [
    "Here we are going to try simple hand writing digits images (MNIST) in small size of gray images. Later on, we will move to use colorful and and more complicated images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-middle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size_train = 10\n",
    "batch_size_test = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('~/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('~/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-policy",
   "metadata": {},
   "source": [
    "Now we should check the data before we define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get minibatch\n",
    "x,target = next(iter(train_loader)); x_test,_ = next(iter(test_loader))\n",
    "print('Loaded image shape in tensor [N,C,H,W]:', x.size())\n",
    "fig,axes = plt.subplots(1,2); \n",
    "# Here is the code to change colormap\n",
    "# If you want to see the intensity of pixel in a better way, use plt.set_cmap(['gray','viridis'][1]);\n",
    "plt.set_cmap(['gray','viridis'][0]);\n",
    "axes[0].imshow(x[0][0].numpy()); axes[1].imshow(x_test[0][0].detach().numpy());\n",
    "axes[0].set_title('Train'); axes[1].set_title('Test');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the simplest way to feed images to AE by flat 2D to 1D\n",
    "input_size = 784\n",
    "\n",
    "# Hyperparameters\n",
    "encoding_dim = 32\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "n_epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d560f94",
   "metadata": {},
   "source": [
    "Now let's create a simple encoder that consists one linear layer with ReLU activation as well as one decoder that consists one linear layer. Then we create an autoencoder that consists one encoder and one decoder. Pay attention that we will use sigmoid function to make sure the reconstructed pixels are within normalized range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, encoding_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(input_size, encoding_dim), \n",
    "             nn.ReLU()])\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encoding_dim, input_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(encoding_dim, input_size)])\n",
    "    \n",
    "    def forward(self,z):\n",
    "        for layer in self.layers:\n",
    "            z = layer(z)\n",
    "        return z\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size = 784, encoding_dim = 32):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_size, encoding_dim)\n",
    "        self.decoder = Decoder(encoding_dim, input_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # flatten to (nm, 1) vector\n",
    "        x = self.encoder(x) # here we get the latent z\n",
    "        x = self.decoder(x) # here we get the reconsturcted input\n",
    "        x = torch.sigmoid(x)\n",
    "        x = x.reshape(x.size(0), 1,28,28) # reshape this flatten vector to the original image size    \n",
    "        return x\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we creat and initialize a autoencoder\n",
    "autoencoder = Autoencoder(784,32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-slope",
   "metadata": {},
   "source": [
    "Here is what the initial autoencoder generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,2); plt.set_cmap(['gray','viridis'][0]);\n",
    "axes[0].imshow(x[0][0].numpy()); axes[0].set_title('Original'); \n",
    "axes[1].imshow(autoencoder(x)[0][0].detach().numpy()); axes[1].set_title('Reconstructed');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142100c0",
   "metadata": {},
   "source": [
    "Addition materials about ploting images in grid so we can see more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b38a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can also use make_grid function to plot images\n",
    "# In Pytorch, the shape of images are (N,C,H,W) \n",
    "# make_grid return arrays in (C, Hs, Ws)\n",
    "# In matplotlib, we need (H,W,C)\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "ims = make_grid(x, nrow=5).permute(1,2,0).numpy()\n",
    "plt.imshow(ims); plt.title('Original images')\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstruct_ims = autoencoder(x).cpu()\n",
    "\n",
    "ims_rec = make_grid(reconstruct_ims, nrow=5).permute(1,2,0).numpy()\n",
    "plt.figure()\n",
    "plt.imshow(ims_rec); plt.title('Reconstructed images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-bubble",
   "metadata": {},
   "source": [
    "Now we move to define how our autoencoder should be trainned. In this function of `train` we directly include the test data process at the end of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader = None, valloader = None, num_epochs = 1):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # name dataloaders for phrase\n",
    "    phases = ['train']\n",
    "    dataloaders = {'train':trainloader}\n",
    "    if valloader:\n",
    "        phases.append('valid')\n",
    "        dataloaders['valid'] = valloader\n",
    "        \n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adadelta(autoencoder.parameters())\n",
    "    criterion = torch.nn.BCELoss() #binary cross entropy \n",
    "    \n",
    "    train_hist, valid_hist = list(),list()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}\\n{\"-\"*10}')\n",
    "        \n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss, running_correct, count = 0.0, 0, 0\n",
    "            for batch_idx, (x, y) in enumerate(dataloaders[phase]):\n",
    "                x,y = x.to(device), y.to(device)\n",
    "\n",
    "                # zero param gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward: track history if training phase\n",
    "                with torch.set_grad_enabled(phase=='train'): # pytorch >= 0.4\n",
    "                    outputs = model(x)\n",
    "                    loss    = criterion(outputs, x)\n",
    "                    # backward & optimize if training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # stats\n",
    "                running_loss += loss.item() * x.size(0)\n",
    "                count += len(x)\n",
    "            \n",
    "            epoch_loss = running_loss / count\n",
    "            \n",
    "            exec('%s_hist.append(%d)' % (phase, epoch_loss))\n",
    "            print(f'{phase} loss {epoch_loss:.6f}')\n",
    "        print()\n",
    "    plt.plot(train_hist, label='train_loss')\n",
    "    plt.plot(valid_hist,label='valid_loss')\n",
    "    plt.legend()    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-specification",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train(autoencoder, trainloader=train_loader, valloader=test_loader, num_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-sympathy",
   "metadata": {},
   "source": [
    "Now we can plot the result of autoencoder to see how closs these two images are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,_ = next(iter(test_loader))\n",
    "#z = autoencoder(x)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "x = x.to(device)\n",
    "with torch.no_grad():\n",
    "    reconstruct_ims = autoencoder(x).cpu()\n",
    "\n",
    "ims_ori = make_grid(x.cpu(), nrow=20).permute(1,2,0).numpy()\n",
    "plt.imshow(ims_ori)\n",
    "plt.title('Original images')\n",
    "\n",
    "plt.figure()\n",
    "ims = make_grid(reconstruct_ims, nrow=20).permute(1,2,0).numpy()\n",
    "plt.imshow(ims)\n",
    "plt.title('Reconstructed images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-graphic",
   "metadata": {},
   "source": [
    "## Limitation of autoencoder\n",
    "In the above example, our encoder and decoder architectures have only one layer without non-linearity. Such encoder-deocoder represent linear transformations (the sigmoid function can be removed since it is simply the function to accelerate the training). It is worth mentioning that Principle Componet Anaysis (PCA) defines one of the soltuions (basis) that satisfied the minimum of reconstruction error. For our autoencoder, indeed, can have several basis that describe the same optimal subspace. It ends up that the new features do not have to be independent.\n",
    "\n",
    "With deeper and more complex architectures, the more the autoencoder can processed to high dimensionality reduction while keeping reconstruction loss low. It invokes an impoartant issue in traditional autoencoder: **the lack of interpretable and exploitable structures in the latent space**. Beside, once the autoencoder has been trained, we still have no way to produce any new content based on the trained latent space. \n",
    "\n",
    "Key to solve this issue: get the distribution of latent space so we can generate samples from this space ==> **Variational Autoencoders (VAE)**\n",
    "![](AE_VAE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721f862f",
   "metadata": {},
   "source": [
    "### Task 1:\n",
    "Change your encoder and decoder to use 2D convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01587e47",
   "metadata": {},
   "source": [
    "## Part 2: Variational Autoencoder\n",
    "*Credits goes to William Falcon from NYU who created this nice [code](https://colab.research.google.com/drive/1_yGmk8ahWhDs23U4mpplBFa-39fsEJoT?usp=sharing#scrollTo=EYDKIsTtk3hJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aabaac",
   "metadata": {},
   "source": [
    "### Objective\n",
    "The objective of VAE is to have a vector $z$ in a high-dimensional space $Z$ which we can easily sample according to some probability density distribution $P(z)$ defined over $Z$. In addition, we want to train parameters ($\\theta$) such that the function $f(z;\\theta)$ will be like the input dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bf656f",
   "metadata": {},
   "source": [
    "In another word, we sample a value $z$ from a probability distribution $P(z|x)$ and we reconstruct a data $\\hat{x}$ by passing z to a decoder $d$. The goal is to make the $\\hat{x}=d(z)$ and $x$ as close as possible. In VAE, we estimate a distribution $P(x|z,\\theta)$. Therefore, we are aiming maximizing the probability of each $X$ under the entire generative process called Maximum likelihood\n",
    "$$P(x)=\\int{P(x|z,\\theta)P(z)dz}$$\n",
    "However, it is intractable. The solution is proposed by [Kingma et Welling 2014](https://arxiv.org/abs/1312.6114). The final derivative of loss function for VAE is:\n",
    "$$\n",
    "\\color{red}{\\min \\mathbb{E}_{q}[ \\log q(z|x) - \\log p(z)]} - \\color{blue}{\\mathbb{E}_{q} \\log p(x|z)}\n",
    "$$\n",
    "Here the blue term is the reconstructed error and the red term is the KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e7ec49",
   "metadata": {},
   "source": [
    "The following usage of Pytorch lightning is optional but recommend to accelerate your learning progress\n",
    "[Pytorch lightning Bolts](https://lightning-bolts.readthedocs.io/en/stable/) is a very useful tool for getting prebuits models across many reserach domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d9754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might not use pytorch-lightning if you have installation issues. A known issue is between python 3.6 and gym\n",
    "# If you get error \"module 'contextlib' has no attribute 'nullcontext'\",\n",
    "# You have a incompatable issue between python and gym. Try to update to python 3.7\n",
    "\n",
    "# Pytorch-lightning will accelearte the training by automatically access GPU or TPU. \n",
    "# !pip install pytorch-lightning==1.0.8\n",
    "%pip install pytorch-lightning\n",
    "\n",
    "# Pytorch lightning Bolts is a collection of prebuilt models across many reserach domains\n",
    "%pip install wandb\n",
    "%pip install gym\n",
    "%pip install pytorch-lightning-bolts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee43c035",
   "metadata": {},
   "source": [
    "We are going to use CIFAR10 images (6000 colorful images in 3x32x32 pixels and 10 different classes). To enable the network in learning this kind of problem, we need to use a better layer instead of Linear. Here we will use [resnet18](https://arxiv.org/pdf/1512.03385.pdf) (deep residual networks with 18 layers pre-trained on ImageNet) to accelerate our learning. Be aware, images from ImageNet is 3x224x224 and images from Cifar are only 3x32x32. This is the reason we disable first layer and it's maxpooling.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eec8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from pl_bolts.models.autoencoders.components import (\n",
    "    resnet18_decoder,\n",
    "    resnet18_encoder,\n",
    ")\n",
    "\n",
    "# Use pytorch_lightning dataloader. This class normalize images with parameter (mu=0.5, std=0.5)\n",
    "# Random divided images to 45000 and 5000 for training and validation\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b45459",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = CIFAR10DataModule(\".\", normalize=True)\n",
    "dm.prepare_data()\n",
    "dm.setup(\"fit\")\n",
    "dataloader = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e41b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = next(iter(dataloader))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472f7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 10, figsize=(10, 1))\n",
    "for i in range(10):  \n",
    "  axes[i].imshow(np.transpose(unnormalize(X[i]), (1, 2, 0)))\n",
    "  axes[i].get_xaxis().set_visible(False)\n",
    "  axes[i].get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ac5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(pl.LightningModule):\n",
    "    def __init__(self, enc_out_dim=512, latent_dim=256, input_height=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # encoder, decoder\n",
    "        self.encoder = resnet18_encoder(False, False) #arguments (first_conv, maxpool1)\n",
    "        self.decoder = resnet18_decoder(\n",
    "            latent_dim=latent_dim, \n",
    "            input_height=input_height, \n",
    "            first_conv=False, \n",
    "            maxpool1=False\n",
    "        )\n",
    "\n",
    "        # distribution parameters\n",
    "        self.fc_mu = nn.Linear(enc_out_dim, latent_dim)\n",
    "        self.fc_var = nn.Linear(enc_out_dim, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7259e3",
   "metadata": {},
   "source": [
    "Now we start to sample z. Beaware, pytorch lightning provides more concise way to use VAE and training. For the education purposes, we are adding the training step to the lightning module instead of the original class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc3c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e19bb5",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "For educational purpose, we will show you the variables and procedure step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c820edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretend we have an colorful image (3 chanels, 32x32 pixels)\n",
    "#x = torch.randn(1,3,32,32)\n",
    "x = X[[0]]\n",
    "print(x.shape)\n",
    "#Get q(z|x) parameters\n",
    "x_encoded = vae.encoder(x)\n",
    "print('Encoded x shape:', x_encoded.shape)\n",
    "\n",
    "# Now we use the final fully connected layer of encoder to get the parameters (mu, var) of q(z|x) distribution \n",
    "mu, log_var = vae.fc_mu(x_encoded),vae.fc_var(x_encoded)\n",
    "print('mu:', mu.shape)\n",
    "print('log_var:',log_var.shape)\n",
    "\n",
    "# sample z from q(z|x)\n",
    "std = torch.exp(log_var/2)\n",
    "q = torch.distributions.Normal(mu, std)\n",
    "z = q.rsample()\n",
    "\n",
    "print('z shape:', z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405b7e45",
   "metadata": {},
   "source": [
    "### decoding\n",
    "The decoder is going to take sampled z as input and it will generate parameters for a distribution from which to sample a reconstruction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c0f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = vae.decoder(z)\n",
    "print('x_hat shape:', x_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c733c033",
   "metadata": {},
   "source": [
    "#### Reconstructed loss\n",
    "You might see other open-source code of VAE use MSE as reconstructed loss. However, pay attention that it is only valid when the images are gray (binary variables). The parameters of decoder ouput in that case will be bernoullis distribution and thus we can use the pixel values to calculate the cross entropy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c3409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_loss = F.mse_loss(x_hat, x, reduction='none')\n",
    "recon_loss = recon_loss.sum(-1).sum(-1).sum(-1)\n",
    "print('Reconstructed loss uing cross entropy (wrong one in our case!):', recon_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1553aec",
   "metadata": {},
   "source": [
    "When we deal with data that has more than one channels (in our example, 3 channels), it is preferred to model each parameter out of the decoder as a gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec83768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the probability of x under this n-dimensional distribution\n",
    "log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "scale = torch.exp(log_scale)\n",
    "# p(x|z)\n",
    "dist = torch.distributions.Normal(x_hat, scale)\n",
    "# Reconstructed loss: Exp_q(log(p(x|z)))\n",
    "log_pxz = dist.log_prob(x)\n",
    "print('shape of log(p(x|z):',log_pxz.shape)\n",
    "# We use Monte carlo sampling theory to do Exp: \n",
    "# Sum across all the channels and pixels\n",
    "log_pxz = log_pxz.sum(dim=(1,2,3))\n",
    "print('Reconstructed error:', log_pxz.item()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04737e52",
   "metadata": {},
   "source": [
    "#### KL divergence\n",
    "Here we use monte carlo estimate.\n",
    "\n",
    "$$\n",
    "= \\mathbb{E}_{q} [\\log q(z|x) - \\log{p(z)}]\\\\\n",
    "=\\mathbb{\\sum}[\\log q(z_i|x)-\\log{p(z_i)}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f5f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. define the first two probabilities (in this case Normal for both)\n",
    "p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "# 2. get the probabilities from the equation\n",
    "log_qzx = q.log_prob(z)\n",
    "log_pz = p.log_prob(z)\n",
    "\n",
    "print('Shape of q(z|x):',log_qzx.shape)\n",
    "print('Shape of p(z):', log_pz.shape)\n",
    "\n",
    "# 3. calculate the KL\n",
    "kl = (log_qzx - log_pz)\n",
    "\n",
    "# 4. this bit is a bit tricky. Since these are log probabilities\n",
    "# we can sum all the individual dimensions to give us the multi-dimensional\n",
    "# probability\n",
    "kl = kl.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e02a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final ELBO loss:\n",
    "elbo = (kl - recon_loss)\n",
    "print('kl:', kl.mean())\n",
    "print('log likelihood (reconstruction loss):', recon_loss.mean())\n",
    "print('elbo: ', elbo.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7016ef91",
   "metadata": {},
   "source": [
    "Here we create a plot function that can visualize the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb45fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_original_generated(x, x_hat, normalize):\n",
    "    mean, std = np.array(normalize.mean), np.array(normalize.std) \n",
    "    x_com = torch.cat((x,x_hat),0).detach()\n",
    "    grid =torchvision.utils.make_grid(x_com).permute(1, 2, 0).numpy() * std + mean\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa36048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = images_original_generated(x,x_hat,cifar10_normalization())\n",
    "plt.imshow(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376cb444",
   "metadata": {},
   "source": [
    "Now we can assemble all the functions into one class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de05b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full implementation of VAE\n",
    "import torch\n",
    "\n",
    "class VAE(pl.LightningModule):\n",
    "    def __init__(self, enc_out_dim=512, latent_dim=256, input_height=32):\n",
    "        super().__init__()\n",
    "        self.img_dim = (3,32,32) #RGB images\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # encoder, decoder\n",
    "        self.encoder = resnet18_encoder(False, False)\n",
    "        self.decoder = resnet18_decoder(\n",
    "            latent_dim=latent_dim, \n",
    "            input_height=input_height, \n",
    "            first_conv=False, \n",
    "            maxpool1=False\n",
    "        )\n",
    "\n",
    "        # distribution parameters\n",
    "        self.fc_mu = nn.Linear(enc_out_dim, latent_dim)\n",
    "        self.fc_var = nn.Linear(enc_out_dim, latent_dim)\n",
    "\n",
    "        # for the gaussian likelihood\n",
    "        self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(),lr = 1e-4)\n",
    "\n",
    "    # Reconstruction loss: P(x|z)\n",
    "    def gaussian_likelihood(self, mean, logscale, sample):\n",
    "        scale = torch.exp(logscale)\n",
    "        dist = torch.distributions.Normal(mean, scale)\n",
    "        log_pxz = dist.log_prob(sample)\n",
    "        return log_pxz.sum(dim=(1, 2, 3))\n",
    "\n",
    "    # sum(log(q(z|x)-log(p(z)))\n",
    "    def kl_divergence(self, z, mu, std):\n",
    "        # 1. define the first two probabilities (in this case Normal for both)\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        # 2. get the probabilities from the equation\n",
    "        log_qzx = q.log_prob(z)\n",
    "        log_pz = p.log_prob(z)\n",
    "\n",
    "        # kl\n",
    "        kl = (log_qzx - log_pz)\n",
    "        kl = kl.sum(-1)\n",
    "        return kl\n",
    "    \n",
    "    #def forward(self,z):\n",
    "    #    img = self.decoder(z)\n",
    "    #    img = img.view(img.size(0),*self.img_dim)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x,_=batch\n",
    "        # encode x to get the mu and variance parameters\n",
    "        x_encoded = self.encoder(x)\n",
    "        mu, log_var = self.fc_mu(x_encoded), self.fc_var(x_encoded)\n",
    "\n",
    "        #sample z from q(z|x)\n",
    "        std = torch.exp(log_var/2)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        z = q.rsample()\n",
    "\n",
    "        #decoded\n",
    "        x_hat = self.decoder(z)\n",
    "\n",
    "        # reconstruction loss\n",
    "        recon_loss = self.gaussian_likelihood(x_hat, self.log_scale, x)\n",
    "\n",
    "        #kl\n",
    "        kl = self.kl_divergence(z, mu, std)\n",
    "\n",
    "        #elbo\n",
    "        elbo = (kl - recon_loss)\n",
    "        elbo = elbo.mean()\n",
    "\n",
    "        self.log_dict({\n",
    "            'elbo': elbo,\n",
    "            'reconstruction': recon_loss.mean(),\n",
    "            'kl': kl.mean(),\n",
    "        })\n",
    "\n",
    "        # log sampling images\n",
    "        grid = images_original_generated(x,x_hat,cifar10_normalization())\n",
    "        self.logger.experiment.add_image(\"generated images-{epoch:02d}\", grid, 0)\n",
    "        \n",
    "        return elbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e69c7",
   "metadata": {},
   "source": [
    "Now let's use CIFAR-10 provided from pl_bolts and start to train our VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016476cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"temp\",exist_ok = True)\n",
    "datamodule = CIFAR10DataModule('temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59969409",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(1234)\n",
    "\n",
    "vae = VAE()\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=18, progress_bar_refresh_rate=10)\n",
    "\n",
    "trainer.fit(vae, datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89978cec",
   "metadata": {},
   "source": [
    "Visualize the resutls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbce43fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59b62b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow, figure\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
    "figure(figsize=(8, 3), dpi=300)\n",
    "\n",
    "# Z COMES FROM NORMAL(0, 1)\n",
    "num_preds = 16\n",
    "p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "z = p.rsample((num_preds,))\n",
    "\n",
    "# SAMPLE IMAGES\n",
    "with torch.no_grad():\n",
    "    pred = vae.decoder(z.to(vae.device)).cpu()\n",
    "\n",
    "# UNDO DATA NORMALIZATION\n",
    "normalize = cifar10_normalization()\n",
    "mean, std = np.array(normalize.mean), np.array(normalize.std)\n",
    "img = make_grid(pred).permute(1, 2, 0).numpy() * std + mean\n",
    "\n",
    "# PLOT IMAGES\n",
    "imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e73bbae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da92ad36b5ec4cf4fc00eb02ec09133b6a7a2fd13d12e637916ab2393c8cce11"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
